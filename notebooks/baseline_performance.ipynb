{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8735ee57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprompt_generators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_basic_prompt, generate_coding_prompt, generate_prompt_with_image, generate_code_fixing_prompt, create_reasoning_model_prompt\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_llm_response, initialize_client\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from prompt_generators import create_basic_prompt, generate_coding_prompt, generate_prompt_with_image, generate_code_fixing_prompt, create_reasoning_model_prompt\n",
    "from llm_utils import generate_llm_response, initialize_client\n",
    "from eval import extract_solution, evaluate_solution, convert_output_to_array, extract_code, execute_transform_function\n",
    "\n",
    "import os\n",
    "\n",
    "import ast\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "     \n",
    "config = {\n",
    "    'model': 'pixtral-large-latest',\n",
    "    'solution_type': 'simple'\n",
    "}\n",
    "\n",
    "client = initialize_client('mistral')\n",
    "print(\"Client initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC data complete.\n",
      "Test set has 300 tasks in it.\n"
     ]
    }
   ],
   "source": [
    "# Create test_set\n",
    "check_dataset()\n",
    "train, eval = load_files_from_json()\n",
    "test_set = filter_tasks_by_grid_size(train)\n",
    "print(f\"Test set has {len(test_set)} tasks in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f151ecdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#   Used this for oneshot simple prompt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (task_name, task_content) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_set\u001b[49m\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#   Used this for oneshot simple prompt\n",
    "#######################################\n",
    "results = []\n",
    "\n",
    "for i, (task_name, task_content) in enumerate(test_set.items()):\n",
    "\n",
    "    print(f\"Processing task {task_name}\")\n",
    "    print(f\"#{i +1} out of {len(test_set)}\")\n",
    "    \n",
    "    messages = create_basic_prompt(task=task_content, allow_reasoning=False)\n",
    "    \n",
    "    print(\"Asking llm for answer!\")\n",
    "    response = generate_llm_response(messages=messages, \n",
    "                                     client=client,\n",
    "                                     model=config['model'],\n",
    "                                     num_samples=1,\n",
    "                                     temperature=0.1)\n",
    "    \n",
    "    print(\"Evaluating answer\")\n",
    "    \n",
    "    extracted_answer = None     # Initialize to None before the try block\n",
    "    try:\n",
    "        extracted_answer_temp = extract_solution(response)            \n",
    "        extracted_answer = convert_output_to_array(extracted_answer_temp)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting extracted solution to NumPy array: {e}\")\n",
    "        extracted_answer = None\n",
    "    \n",
    "    correct_solution = np.array(task_content['test'][0]['output'])\n",
    "    result = evaluate_solution(extracted_answer, correct_solution)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    row = (\n",
    "        {\n",
    "            'task': task_name,\n",
    "            'llm_full_answer': response,\n",
    "            'llm_extracted_answer': extracted_answer\n",
    "        })\n",
    "    \n",
    "    row.update(result)\n",
    "    results.append(row)\n",
    "    \n",
    "    # to not exceed usage rates\n",
    "    print(\"Waiting for one second...\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"results/vl_models/Pixtral-Large-123B_baseline.csv\", index=False)\n",
    "    print(f\"Data saved to CSV after {i + 1} iterations.\")\n",
    " \n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"results/vl_models/Pixtral-Large-123B_baseline.csv\", index=False)\n",
    "print(\"Data saved to CSV at the end of the loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698308c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task                    36\n",
       "llm_full_answer         36\n",
       "llm_extracted_answer    36\n",
       "answer_extracted        36\n",
       "correct_grid_size       36\n",
       "percentage_correct      36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"percentage_correct == 1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
